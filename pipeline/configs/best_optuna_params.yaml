# Best Hyperparameter Configuration from Optuna Tuning
# Trial 74: R@1 = 11.45% (Best result from 100 trials)
# Generated from hyperparameter tuning on 100K subset

# Model architecture configuration
model:
  type: attention                    # Attention projection head performed best
  output_dim: 1024                  # High dimensional output space
  hidden_dim: 512                   # Internal hidden dimension
  dropout: 0.004132                 # Very low dropout (minimal regularization needed)
  
  # Attention-specific parameters
  num_heads: 16                     # Many attention heads for rich representations
  num_layers: 1                     # Single layer is sufficient
  
# Loss function configuration  
loss:
  type: softmax_infonce            # Softmax InfoNCE outperformed sigmoid and queue variants
  temperature: 0.03817             # Low temperature for sharp similarity distributions

# Training configuration
training:
  # Hyperparameters from tuning
  batch_size: 4096                 # Large batch size for stable gradients
  learning_rate: 0.0001106         # Conservative learning rate
  weight_decay: 0.000005939        # Minimal weight decay
  
  # Training schedule (for full training)
  num_epochs: 50                   # Full training epochs (vs 8 in tuning)
  warmup_epochs: 5                 # Learning rate warmup
  patience: 10                     # Early stopping patience
  max_grad_norm: 1.0              # Gradient clipping
  
  # Optimizer settings
  optimizer: adamw
  betas: [0.9, 0.999]
  eps: 1.0e-8
  
  # Learning rate schedule
  scheduler: cosine_annealing
  min_lr: 1.0e-6
  
# Dataset configuration
data:
  # Use full dataset for final training (not tuning subset)
  train_image_path: "train_image_embeddings.pt"     # Full 473K samples
  train_text_path: "train_text_embeddings.pt"
  train_metadata_path: "train_metadata.json"
  
  val_image_path: "val_image_embeddings.pt"
  val_text_path: "val_text_embeddings.pt" 
  val_metadata_path: "val_metadata.json"
  
  # Data loading
  num_workers: 4
  pin_memory: true
  
# Evaluation configuration
evaluation:
  # Metrics to compute
  metrics: ['recall_1', 'recall_5', 'recall_10', 'mean_rank', 'median_rank']
  
  # Evaluation frequency
  eval_every_n_epochs: 1
  save_best_model: true
  
# Logging and checkpointing
logging:
  log_every_n_steps: 100
  save_every_n_epochs: 5
  checkpoint_dir: "checkpoints/best_hyperparams"
  
# Hardware configuration
device:
  use_cuda: true
  mixed_precision: true           # Enable for faster training
  compile_model: false            # PyTorch 2.0 compilation (optional)

# Expected performance (based on tuning results)
# Tuning R@1: 11.45% (on 100K subset, 8 epochs)  
# Expected final R@1: 25-40% (on full dataset, 50 epochs)