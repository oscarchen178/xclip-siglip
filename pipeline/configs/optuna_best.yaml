# Best Hyperparameter Configuration from Optuna Tuning
# Trial 74: R@1 = 11.45% (Best result from 100 trials)
# Generated from hyperparameter tuning on 100K subset
experiment_name: "optuna_best"
output_dir: "results"
device: "auto"
seed: 42

# Data paths
data:
  train_image_embeddings: "train_image_embeddings.pt"
  train_text_embeddings: "train_text_embeddings.pt"
  val_image_embeddings: "val_image_embeddings.pt"
  val_text_embeddings: "val_text_embeddings.pt"
  test_image_embeddings: "test_image_embeddings.pt"
  test_text_embeddings: "test_text_embeddings.pt"

# Model architecture (attention head performed best)
model:
  type: "attention"
  output_dim: 1024
  hidden_dim: 512
  dropout: 0.004132
  num_heads: 16
  num_layers: 1

# Training settings
training:
  batch_size: 4096
  learning_rate: 0.0001106
  weight_decay: 0.000005939
  num_epochs: 50
  patience: 10
  max_grad_norm: 1.0

# Loss function (softmax InfoNCE performed best)
loss:
  type: "softmax_infonce"
  temperature: 0.03817

# Evaluation settings
evaluation:
  top_k: [1, 5, 10, 50]
  visualization_samples: 5000
  tsne_perplexity: 30
  save_features: true

# Expected performance (based on tuning results)
# Tuning R@1: 11.45% (on 100K subset, 8 epochs)  
# Expected final R@1: 25-40% (on full dataset, 50 epochs)