# Cross-Modal Projection Head Training Configuration
# Edit this file to configure all aspects of training and evaluation

# Experiment settings
experiment_name: "softmax_infonce_baseline"
output_dir: "results"
device: "auto"  # auto, cuda, mps, cpu
seed: 42

# Data paths
data:
  # Training split (80% of train2017, ~95K samples)
  train_image_embeddings: "train_image_embeddings.pt"
  train_text_embeddings: "train_text_embeddings.pt"
  # Validation set (val2017, ~5K samples) 
  val_image_embeddings: "val_image_embeddings.pt"
  val_text_embeddings: "val_text_embeddings.pt"
  # Test set (20% of train2017, ~24K samples)
  test_image_embeddings: "test_image_embeddings.pt"
  test_text_embeddings: "test_text_embeddings.pt"

# Model architecture
model:
  type: "clip"  # Options: siglip, clip, attention
  output_dim: 512
  hidden_dim: 1024
  dropout: 0.1
  learnable_temp: false

# Training settings
training:
  batch_size: 1024
  learning_rate: 1e-4
  weight_decay: 0.01
  num_epochs: 50
  patience: 5  # early stopping patience
  max_grad_norm: 1.0
  
# Loss function
loss:
  type: "softmax_infonce"  # Options: sigmoid_infonce, softmax_infonce, queue_infonce, clip
  temperature: 0.07
  learnable_scale: false   # Use fixed temperature from config (set to true for learnable)

# Evaluation settings
evaluation:
  top_k: [1, 5, 10, 50]
  visualization_samples: 5000
  tsne_perplexity: 30
  save_features: true