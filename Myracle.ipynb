{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and encode sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python - <<'PY'\n",
    "import sentencepiece as spm\n",
    "print(\"SentencePiece version:\", spm.__version__)\n",
    "# PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COCO captions metadata ...\n",
      "Sampled 100 image-caption pairs.\n",
      "Loading image encoder ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a35160824748d48c19ec4493253fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nSiglipTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 150\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampled \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sampled_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m image-caption pairs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading image encoder ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m image_processor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_MODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m image_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    152\u001b[0m     IMAGE_MODEL_NAME,\n\u001b[1;32m    153\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mTORCH_DTYPE_PREF,\n\u001b[1;32m    154\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading text encoder ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ece1786/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py:328\u001b[0m, in \u001b[0;36mAutoProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processor_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    325\u001b[0m         pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m processor_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocessor_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Last try: we use the PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m PROCESSOR_MAPPING:\n",
      "File \u001b[0;32m~/miniconda3/envs/ece1786/lib/python3.10/site-packages/transformers/processing_utils.py:974\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 974\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ece1786/lib/python3.10/site-packages/transformers/processing_utils.py:1020\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m-> 1020\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/miniconda3/envs/ece1786/lib/python3.10/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ece1786/lib/python3.10/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nSiglipTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sample 100 MS-COCO val2017 images + captions and encode them with:\n",
    "  Image encoder:  google/siglip2-giant-opt-patch16-256\n",
    "  Text  encoder:  intfloat/e5-mistral-7b-instruct\n",
    "\n",
    "Assumed directory layout (relative to this script or working directory):\n",
    "\n",
    "Data/\n",
    "  val2017/                # COCO val2017 images *.jpg\n",
    "  annotations/\n",
    "    captions_val2017.json # COCO caption annotations\n",
    "\n",
    "Outputs:\n",
    "  outputs/sample_image_ids.json\n",
    "  outputs/image_embeddings.pt          (tensor [N, D_img])\n",
    "  outputs/text_embeddings.pt           (tensor [N, D_txt])\n",
    "  outputs/pairs.parquet                (optional metadata)\n",
    "\n",
    "Notes:\n",
    "  * The SigLIP2 giant + Mistral 7B models are large; you likely need a >=24GB GPU (or multiple) for full precision.\n",
    "  * The code tries to use bfloat16/float16 and device_map='auto'. If you face OOM, lower batch_size or enable 8-bit loading (commented section).\n",
    "  * For simplicity we pick ONE caption per image (the first), but you can adapt to keep all.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import json, random, math, os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "# ----------------------------- Configuration -------------------------------- #\n",
    "DATA_ROOT = Path(\"Data\")\n",
    "IMAGES_DIR = DATA_ROOT / \"val2017\"\n",
    "CAPTIONS_FILE = DATA_ROOT / \"annotations\" / \"captions_val2017.json\"\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NUM_SAMPLES = 100            # number of image-caption pairs to sample\n",
    "SEED = 42\n",
    "IMAGE_MODEL_NAME = \"google/siglip2-giant-opt-patch16-256\"\n",
    "TEXT_MODEL_NAME  = \"intfloat/e5-mistral-7b-instruct\"\n",
    "IMAGE_BATCH_SIZE = 8         # adjust if you hit OOM\n",
    "TEXT_BATCH_SIZE  = 4         # adjust if you hit OOM\n",
    "TORCH_DTYPE_PREF = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else (\n",
    "    torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# If you want 8-bit quantization for the text model (requires bitsandbytes), uncomment below:\n",
    "# LOAD_TEXT_IN_8BIT = True\n",
    "LOAD_TEXT_IN_8BIT = False\n",
    "\n",
    "# ----------------------------- Utilities ------------------------------------ #\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def load_coco_captions(captions_path: Path) -> Dict[int, List[str]]:\n",
    "    with open(captions_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    id_to_captions: Dict[int, List[str]] = {}\n",
    "    for ann in data[\"annotations\"]:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        id_to_captions.setdefault(img_id, []).append(ann[\"caption\"].strip())\n",
    "    return id_to_captions, {img['id']: img for img in data[\"images\"]}\n",
    "\n",
    "\n",
    "def sample_image_ids(all_image_meta: Dict[int, dict], k: int) -> List[int]:\n",
    "    all_ids = list(all_image_meta.keys())\n",
    "    random.shuffle(all_ids)\n",
    "    return all_ids[:k]\n",
    "\n",
    "\n",
    "def load_image(path: Path) -> Image.Image:\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "# ----------------------------- Main Encoding Logic -------------------------- #\n",
    "@torch.no_grad()\n",
    "def encode_images(image_paths: List[Path], image_model, image_processor, batch_size: int) -> torch.Tensor:\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Encoding images\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = [load_image(p) for p in batch_paths]\n",
    "        inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(image_model.device, non_blocking=True) for k, v in inputs.items()}\n",
    "        feats = image_model.get_image_features(**inputs)  # shape [B, D]\n",
    "        feats = torch.nn.functional.normalize(feats, dim=-1)\n",
    "        embs.append(feats.cpu())\n",
    "    return torch.cat(embs, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts(texts: List[str], text_model, tokenizer, batch_size: int) -> torch.Tensor:\n",
    "    embs = []\n",
    "    # E5 instruct models expect a task prefix for queries/passages. For captions treat as 'passage:'\n",
    "    proc_texts = [f\"passage: {t}\" for t in texts]\n",
    "    for i in tqdm(range(0, len(proc_texts), batch_size), desc=\"Encoding texts\"):\n",
    "        batch_txt = proc_texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_txt, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "        inputs = {k: v.to(text_model.device, non_blocking=True) for k, v in inputs.items()}\n",
    "        outputs = text_model(**inputs)\n",
    "        # E5 uses last hidden state + attention mask mean pooling\n",
    "        hidden = outputs.last_hidden_state  # [B, L, H]\n",
    "        mask = inputs['attention_mask'].unsqueeze(-1)  # [B, L, 1]\n",
    "        summed = (hidden * mask).sum(dim=1)\n",
    "        counts = mask.sum(dim=1)\n",
    "        sentence_emb = summed / counts\n",
    "        sentence_emb = torch.nn.functional.normalize(sentence_emb, dim=-1)\n",
    "        embs.append(sentence_emb.cpu())\n",
    "    return torch.cat(embs, dim=0)\n",
    "\n",
    "# ----------------------------- Execution ------------------------------------ #\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(SEED)\n",
    "\n",
    "    if not CAPTIONS_FILE.exists():\n",
    "        raise FileNotFoundError(f\"Cannot find captions file at {CAPTIONS_FILE}\")\n",
    "    if not IMAGES_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Cannot find images directory at {IMAGES_DIR}\")\n",
    "\n",
    "    print(\"Loading COCO captions metadata ...\")\n",
    "    id_to_caps, id_to_imgmeta = load_coco_captions(CAPTIONS_FILE)\n",
    "    sampled_ids = sample_image_ids(id_to_imgmeta, NUM_SAMPLES)\n",
    "\n",
    "    # Select first caption for each image (customize as needed)\n",
    "    sampled_captions = []\n",
    "    image_paths = []\n",
    "    for img_id in sampled_ids:\n",
    "        rel_name = f\"{img_id:012d}.jpg\"  # COCO filename pattern\n",
    "        img_path = IMAGES_DIR / rel_name\n",
    "        if not img_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing image file {img_path}\")\n",
    "        image_paths.append(img_path)\n",
    "        cap_list = id_to_caps.get(img_id, [\"(no caption)\"])\n",
    "        sampled_captions.append(cap_list[0])\n",
    "\n",
    "    print(f\"Sampled {len(sampled_ids)} image-caption pairs.\")\n",
    "\n",
    "    print(\"Loading image encoder ...\")\n",
    "    image_processor = AutoProcessor.from_pretrained(IMAGE_MODEL_NAME)\n",
    "    image_model = AutoModel.from_pretrained(\n",
    "        IMAGE_MODEL_NAME,\n",
    "        torch_dtype=TORCH_DTYPE_PREF,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    print(\"Loading text encoder ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "    text_model_kwargs = dict(torch_dtype=TORCH_DTYPE_PREF, device_map=\"auto\")\n",
    "\n",
    "    if LOAD_TEXT_IN_8BIT:\n",
    "        # Requires bitsandbytes installed; uncomment if desired.\n",
    "        # from transformers import BitsAndBytesConfig\n",
    "        # quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        # text_model_kwargs.update(dict(quantization_config=quant_config, device_map=\"auto\"))\n",
    "        pass\n",
    "\n",
    "    text_model = AutoModel.from_pretrained(TEXT_MODEL_NAME, **text_model_kwargs)\n",
    "\n",
    "    # Encode\n",
    "    image_embeddings = encode_images(image_paths, image_model, image_processor, IMAGE_BATCH_SIZE)\n",
    "    text_embeddings = encode_texts(sampled_captions, text_model, tokenizer, TEXT_BATCH_SIZE)\n",
    "\n",
    "    print(\"Embeddings shapes:\")\n",
    "    print(\"  Images:\", image_embeddings.shape)\n",
    "    print(\"  Texts :\", text_embeddings.shape)\n",
    "\n",
    "    torch.save(image_embeddings, OUTPUT_DIR / \"image_embeddings.pt\")\n",
    "    torch.save(text_embeddings, OUTPUT_DIR / \"text_embeddings.pt\")\n",
    "\n",
    "    # Save mapping metadata\n",
    "    import json\n",
    "    meta = {\n",
    "        \"image_ids\": sampled_ids,\n",
    "        \"captions\": sampled_captions,\n",
    "        \"image_paths\": [str(p) for p in image_paths],\n",
    "        \"image_embedding_file\": \"image_embeddings.pt\",\n",
    "        \"text_embedding_file\": \"text_embeddings.pt\",\n",
    "        \"image_model\": IMAGE_MODEL_NAME,\n",
    "        \"text_model\": TEXT_MODEL_NAME,\n",
    "        \"seed\": SEED,\n",
    "    }\n",
    "    with open(OUTPUT_DIR / \"sample_image_ids.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        import pyarrow  # noqa: F401\n",
    "        import pyarrow.parquet  # noqa: F401\n",
    "        df = pd.DataFrame({\n",
    "            \"image_id\": sampled_ids,\n",
    "            \"image_path\": [str(p) for p in image_paths],\n",
    "            \"caption\": sampled_captions\n",
    "        })\n",
    "        df.to_parquet(OUTPUT_DIR / \"pairs.parquet\", index=False)\n",
    "        print(\"Saved pairs.parquet\")\n",
    "    except ImportError:\n",
    "        print(\"pandas/pyarrow not installed; skipping parquet export.\")\n",
    "\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
